#!/usr/bin/env python
"""Remove availability of items no longer present in OPDS 2.0 import collections."""
import itertools
import json
from collections.abc import Generator
from typing import Any, cast

from sqlalchemy.orm import raiseload

from palace.manager.core.coverage import CoverageFailure
from palace.manager.core.metadata_layer import TimestampData
from palace.manager.core.opds2_import import OPDS2API, OPDS2Importer, OPDS2ImportMonitor
from palace.manager.scripts.input import CollectionInputScript
from palace.manager.sqlalchemy.model.collection import Collection
from palace.manager.sqlalchemy.model.edition import Edition
from palace.manager.sqlalchemy.model.identifier import Identifier
from palace.manager.sqlalchemy.model.licensing import LicensePool


def main():
    reaper_script = OPDS2ReaperScript(
        importer_class=OPDS2Importer,
        monitor_class=OPDS2ReaperMonitor,
        protocol=OPDS2Importer.NAME,
    )

    reaper_script.run()


class OPDS2ReaperMonitor(OPDS2ImportMonitor):
    """Monitor to make unavailable any license pools without a matching identifier in the feed."""

    # TODO: This should really go in a base `OPDSReaperMonitor` class.
    SERVICE_NAME = "OPDS Reaper Monitor"

    def __init__(
        self,
        *args: Any,
        dry_run: bool = False,
        **import_class_kwargs: Any,
    ) -> None:
        self.seen_identifiers: set[str] = set()
        self.missing_id_count = 0
        self.publication_count = 0
        self.dry_run = dry_run
        super().__init__(*args, **import_class_kwargs)

    def _get_feeds(self) -> Generator[tuple[str, bytes], None, None]:
        """Yield each feed page in the order that it is first fetched.

        This is okay for a reaper, since it has to visit every page
        in the feed and the order is unimportant.
        """
        queue = [cast(str, self.feed_url)]
        seen_links = set()

        # Follow the feed's next links until we reach a page with nothing new.
        while queue:
            new_queue = []
            for link in queue:
                if link in seen_links:
                    continue
                next_links, feed = self.follow_one_link(link)
                new_queue.extend(next_links)
                if feed:
                    yield link, feed
                seen_links.add(link)
            queue = new_queue

    def feed_contains_new_data(self, feed: bytes | str) -> bool:
        # Always return True so that the reaper will crawl the entire feed.
        return True

    def import_one_feed(
        self, feed: bytes | str
    ) -> tuple[list[Edition], dict[str, list[CoverageFailure]]]:
        # Collect all the identifiers in the given feed page.
        feed_obj = json.loads(feed)
        publications: list[dict[str, Any]] = feed_obj["publications"]
        identifiers = list(
            filter(
                None,
                (pub.get("metadata", {}).get("identifier") for pub in publications),
            )
        )

        self.publication_count += len(publications)
        self.missing_id_count += len(publications) - len(identifiers)
        self.seen_identifiers.update(identifiers)

        # No editions / coverage failures, since we're just reaping.
        return [], {}

    def run_once(self, progress: TimestampData) -> TimestampData:
        """Check to see if any identifiers we know about are no longer
        present on the remote. If there are any, remove them.

        :param progress: A TimestampData, ignored.
        """
        super().run_once(progress)
        feed_id_count = len(self.seen_identifiers)
        self.log.info(
            f"Feed contained {self.publication_count} publication entries, "
            f"{feed_id_count} unique identifiers, "
            f"{self.missing_id_count} missing identifiers."
        )

        # Number of ORM objects to buffer at a time.
        query_batch_size = 500

        # Convert feed identifiers to our identifiers, so we can find them.
        # Unlike the import case, we don't want to create identifiers, if
        # they don't already exist.
        self.log.info(
            f"Mapping {feed_id_count} feed identifiers to database identifiers."
        )
        failure_total = 0
        id_looked_up_count = 0
        db_identifiers: dict[str, Identifier] = {}

        feed_id_generator = (id_ for id_ in self.seen_identifiers)
        while _feed_id_batch := list(
            itertools.islice(feed_id_generator, query_batch_size)
        ):
            _batch_size = len(_feed_id_batch)
            _batch_db_ids, _batch_failures = Identifier.parse_urns(
                self._db, _feed_id_batch, autocreate=False
            )
            db_identifiers |= _batch_db_ids
            id_looked_up_count += _batch_size
            _success_count = len(_batch_db_ids)
            _failure_count = len(_batch_failures)
            failure_total += _failure_count
            self.log.info(
                f"Mapped batch of {_batch_size} feed identifier(s) to database identifier(s) "
                f"(cumulative: {id_looked_up_count} of {feed_id_count} feed ids) "
                f"with {_success_count} success(es) and {_failure_count} failure(s))."
            )

        self.log.info(
            f"Successfully mapped {len(db_identifiers)} feed identifier(s) to database identifier(s)."
        )
        if failure_total > 0:
            self.log.warning(
                f"Unable to parse {failure_total} of {feed_id_count} identifiers."
            )

        collection_license_pools_qu = self._db.query(LicensePool).filter(
            LicensePool.collection_id == self.collection.id
        )
        collection_lp_count = collection_license_pools_qu.count()

        eligible_license_pools_qu = collection_license_pools_qu.filter(
            LicensePool.licenses_available == LicensePool.UNLIMITED_ACCESS
        )
        eligible_lp_count = eligible_license_pools_qu.count()

        self.log.info(
            f"{eligible_lp_count} of collection's {collection_lp_count} license pool(s) "
            "are unlimited and eligible to be reaped, if missing from the feed."
        )

        reap_count = 0
        pool: LicensePool
        db_identifier_ids = {x.id for x in list(db_identifiers.values())}

        # Note: We need to turn off eager loading, so that `yield_per` works safely.
        # `raiseload` will let us know if we're accidentally accessing a joined table.
        for pool in eligible_license_pools_qu.options(raiseload("*")).yield_per(
            query_batch_size
        ):
            if pool.identifier_id not in db_identifier_ids:
                reap_count += 1
                # Don't actually reap, unless this is explicitly NOT a dry run.
                if self.dry_run is False:
                    pool.unlimited_access = False

        achievements = (
            f"Dry run: {reap_count} of {eligible_lp_count} eligible license pool(s) would have been marked unavailable. {failure_total} failures parsing identifiers from feed."
            if self.dry_run
            else f"{reap_count} of {eligible_lp_count} eligible license pool(s) marked unavailable. {failure_total} failures parsing identifiers from feed."
        )
        return TimestampData(achievements=achievements)


class OPDS2ReaperScript(CollectionInputScript):
    """Import all books from the OPDS feed associated with a collection."""

    name = "OPDS Reaper Monitor"

    IMPORTER_CLASS = OPDS2Importer
    MONITOR_CLASS: type[OPDS2ReaperMonitor] = OPDS2ReaperMonitor
    PROTOCOL = OPDS2API.label()

    def __init__(
        self,
        _db=None,
        importer_class=None,
        monitor_class=None,
        protocol=None,
        *args,
        **kwargs,
    ):
        super().__init__(_db, *args, **kwargs)
        self.importer_class = importer_class or self.IMPORTER_CLASS
        self.monitor_class = monitor_class or self.MONITOR_CLASS
        self.protocol = protocol or self.PROTOCOL
        self.importer_kwargs = kwargs

    @classmethod
    def arg_parser(cls):
        parser = super().arg_parser()
        parser.add_argument(
            "--dry-run",
            "-n",
            help="Don't actually reap any books. Just report the statistics.",
            dest="dry_run",
            action="store_true",
        )
        parser.add_argument(
            "--all-collections-for-protocol",
            "-a",
            help="Use all collections with associate protocol(self.protocol), if no collections specified..",
            dest="all_protocol_collections",
            action="store_true",
        )
        return parser

    def do_run(self, cmd_args=None) -> None:
        parsed = self.parse_command_line(self._db, cmd_args=cmd_args)
        collections: list[Collection] = parsed.collections
        if collections and parsed.all_protocol_collections:
            self.log.error(
                "Cannot specify both --all-collections-for-protocol and one or more individual collections."
            )
            return

        if not collections and parsed.all_protocol_collections:
            collections = list(Collection.by_protocol(self._db, self.protocol))
        if not collections:
            self.log.error("No collections specified.")
            return

        self.log.info(
            f"Reaping books from {len(collections)} collection{'s' if len(collections) != 1 else ''}."
        )
        for collection in collections:
            self.run_monitor(
                collection,
                dry_run=parsed.dry_run,
            )

    def run_monitor(self, collection, *, dry_run=False):
        monitor = self.monitor_class(
            self._db,
            collection,
            import_class=self.importer_class,
            dry_run=dry_run,
            **self.importer_kwargs,
        )
        monitor.run()


if __name__ == "__main__":
    main()
