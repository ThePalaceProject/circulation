#!/usr/bin/env python
"""Remove availability of items no longer present in OPDS 2.0 import collections."""
import json
from collections.abc import Generator
from typing import Any, cast

from webpub_manifest_parser.opds2 import OPDS2FeedParserFactory

from palace.manager.core.coverage import CoverageFailure
from palace.manager.core.metadata_layer import TimestampData
from palace.manager.core.opds2_import import (
    OPDS2API,
    OPDS2Importer,
    OPDS2ImportMonitor,
    RWPMManifestParser,
)
from palace.manager.scripts.input import CollectionInputScript
from palace.manager.sqlalchemy.model.collection import Collection
from palace.manager.sqlalchemy.model.edition import Edition
from palace.manager.sqlalchemy.model.identifier import Identifier
from palace.manager.sqlalchemy.model.licensing import LicensePool


def main():
    reaper_script = OPDS2ReaperScript(
        importer_class=OPDS2Importer,
        monitor_class=OPDS2ReaperMonitor,
        protocol=OPDS2Importer.NAME,
        parser=RWPMManifestParser(OPDS2FeedParserFactory()),
    )

    reaper_script.run()


class OPDS2ReaperMonitor(OPDS2ImportMonitor):
    """Monitor to make unavailable any license pools without a matching identifier in the feed."""

    # TODO: This should really go in a base `OPDSReaperMonitor` class.
    SERVICE_NAME = "OPDS Reaper Monitor"

    def __init__(
        self,
        *args: Any,
        dry_run: bool = False,
        **import_class_kwargs: Any,
    ) -> None:
        self.seen_identifiers: set[str] = set()
        self.missing_id_count = 0
        self.publication_count = 0
        self.dry_run = dry_run
        super().__init__(*args, **import_class_kwargs)

    def _get_feeds(self) -> Generator[tuple[str, bytes], None, None]:
        """Yield each feed page in the order that it is first fetched.

        This is okay for a reaper, since it has to visit every page
        in the feed and the order is unimportant.
        """
        queue = [cast(str, self.feed_url)]
        seen_links = set()

        # Follow the feed's next links until we reach a page with nothing new.
        while queue:
            new_queue = []
            for link in queue:
                if link in seen_links:
                    continue
                next_links, feed = self.follow_one_link(link)
                new_queue.extend(next_links)
                if feed:
                    yield link, feed
                seen_links.add(link)
            queue = new_queue

    def feed_contains_new_data(self, feed: bytes | str) -> bool:
        # Always return True so that the reaper will crawl the entire feed.
        return True

    def import_one_feed(
        self, feed: bytes | str
    ) -> tuple[list[Edition], dict[str, list[CoverageFailure]]]:
        # Collect all the identifiers in the given feed page.
        feed_obj = json.loads(feed)
        publications: list[dict[str, Any]] = feed_obj["publications"]
        identifiers = list(
            filter(
                None,
                (pub.get("metadata", {}).get("identifier") for pub in publications),
            )
        )

        self.publication_count += len(publications)
        self.missing_id_count += len(publications) - len(identifiers)
        self.seen_identifiers.update(identifiers)

        # No editions / coverage failures, since we're just reaping.
        return [], {}

    def run_once(self, progress: TimestampData) -> TimestampData:
        """Check to see if any identifiers we know about are no longer
        present on the remote. If there are any, remove them.

        :param progress: A TimestampData, ignored.
        """
        super().run_once(progress)
        self.log.info(
            f"Feed contained {self.publication_count} publication entries, "
            f"{len(self.seen_identifiers)} unique identifiers, "
            f"{self.missing_id_count} missing identifiers."
        )

        # Convert feed identifiers to our identifiers, so we can find them.
        # Unlike the import case, we don't want to create identifiers, if
        # they don't already exist.
        identifiers, failures = Identifier.parse_urns(
            self._db, self.seen_identifiers, autocreate=False
        )
        identifier_ids = [x.id for x in list(identifiers.values())]
        if failures:
            self.log.warning(
                f"Unable to parse {len(failures)} of {len(self.seen_identifiers)} identifiers."
            )

        collection_license_pools_qu = self._db.query(LicensePool).filter(
            LicensePool.collection_id == self.collection.id
        )
        collection_license_pools = collection_license_pools_qu.count()

        unlimited_access_license_pools_qu = collection_license_pools_qu.filter(
            LicensePool.licenses_available == LicensePool.UNLIMITED_ACCESS
        )
        unlimited_access_license_pools = unlimited_access_license_pools_qu.count()

        # At this point we've gone through the feed and collected all the identifiers.
        # If there's anything we didn't see, we know it's no longer available.
        to_be_reaped_qu = unlimited_access_license_pools_qu.join(Identifier).filter(
            ~Identifier.id.in_(identifier_ids)
        )
        reap_count = to_be_reaped_qu.count()
        self.log.info(
            f"Reaping {reap_count} of {unlimited_access_license_pools} unlimited (of {collection_license_pools} total) license pools from collection '{self.collection.name}'. "
        )

        if self.dry_run:
            # TODO: Need to prevent timestamp update for dry runs.
            achievements = f"Dry run: {reap_count} license pools would have been removed. Failures parsing identifiers from feed: {len(failures)}."
        else:
            achievements = f"License pools removed: {reap_count}. Failures parsing identifiers from feed: {len(failures)}."
            for pool in to_be_reaped_qu:
                pool.unlimited_access = False
        self.log.info(achievements)

        return TimestampData(achievements=achievements)


class OPDS2ReaperScript(CollectionInputScript):
    """Import all books from the OPDS feed associated with a collection."""

    name = "OPDS Reaper Monitor"

    IMPORTER_CLASS = OPDS2Importer
    MONITOR_CLASS: type[OPDS2ReaperMonitor] = OPDS2ReaperMonitor
    PROTOCOL = OPDS2API.label()

    def __init__(
        self,
        _db=None,
        importer_class=None,
        monitor_class=None,
        protocol=None,
        *args,
        **kwargs,
    ):
        super().__init__(_db, *args, **kwargs)
        self.importer_class = importer_class or self.IMPORTER_CLASS
        self.monitor_class = monitor_class or self.MONITOR_CLASS
        self.protocol = protocol or self.PROTOCOL
        self.importer_kwargs = kwargs

    @classmethod
    def arg_parser(cls):
        parser = super().arg_parser()
        parser.add_argument(
            "--dry-run",
            "-n",
            help="Don't actually reap any books. Just report the statistics.",
            dest="dry_run",
            action="store_true",
        )
        parser.add_argument(
            "--all-collections-for-protocol",
            "-a",
            help="Use all collections with associate protocol(self.protocol), if no collections specified..",
            dest="all_protocol_collections",
            action="store_true",
        )
        return parser

    def do_run(self, cmd_args=None) -> None:
        parsed = self.parse_command_line(self._db, cmd_args=cmd_args)
        collections: list[Collection] = parsed.collections
        if collections and parsed.all_protocol_collections:
            self.log.error(
                "Cannot specify both --all-collections-for-protocol and one or more individual collections."
            )
            return

        if not collections and parsed.all_protocol_collections:
            collections = list(Collection.by_protocol(self._db, self.protocol))
        if not collections:
            self.log.error("No collections specified.")
            return

        self.log.info(f"Reaping books from {len(collections)} collections.")
        for collection in collections:
            self.run_monitor(
                collection,
                dry_run=parsed.dry_run,
            )

    def run_monitor(self, collection, *, dry_run=False):
        monitor = self.monitor_class(
            self._db,
            collection,
            import_class=self.importer_class,
            dry_run=dry_run,
            **self.importer_kwargs,
        )
        monitor.run()


if __name__ == "__main__":
    main()
